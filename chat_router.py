# stream data t·ªõi html
import os
import re
import time
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# LlamaIndex
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.base.llms.types import ChatMessage, MessageRole
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.selectors import LLMSingleSelector

# from llama_index.llms.deepseek import DeepSeek
# from llama_index.llms.groq import Groq
# from llama_index.llms.cerebras import Cerebras
# from llama_index.indices.managed.llama_cloud import LlamaCloudIndex

from my_module import setup_pinecone_vector_index

# Embed model setup
Settings.embed_model = GoogleGenAIEmbedding(
    model_name="gemini-embedding-exp-03-07",
    api_key=os.getenv("GOOGLE_API_KEY")
)

# Chat input model
class ChatInput(BaseModel):
    message: str


def extract_and_format_images(text: str):
    """
    T√¨m v√† format c√°c URL h√¨nh ·∫£nh trong text d·∫°ng (https://...) th√†nh [IMAGE:...]
    ·∫®n link g·ªëc v√† thay b·∫±ng placeholder ·∫£nh cho frontend x·ª≠ l√Ω.
    Returns: (formatted_text, has_images)
    """
    # Regex t√¨m (https://...) trong ngo·∫∑c ƒë∆°n ho·∫∑c ngo√†i
    url_pattern = r'\(?(https?://[^\s\)]+)\)?'

    formatted_text = text
    has_images = False
    found_urls = re.findall(url_pattern, text)

    for url in found_urls:
        # N·∫øu ch∆∞a ph·∫£i ·∫£nh, nh∆∞ng t·ª´ c√°c d·ªãch v·ª• hosting (UploadThing, S3, Cloudinary‚Ä¶)
        if any(domain in url for domain in ["uploadthing", "amazonaws.com", "ufs.sh", "supabase", "cdn", "unsplash"]):
            image_tag = f"[IMAGE:{url}]"
            # Replace ƒëo·∫°n "(https://...)" ho·∫∑c " https://... "
            formatted_text = formatted_text.replace(f"({url})", "")  # x√≥a trong ngo·∫∑c
            formatted_text = formatted_text.replace(url, "")         # x√≥a n·∫øu kh√¥ng ngo·∫∑c
            formatted_text += f"\n\n{image_tag}"
            has_images = True
        elif re.search(r'\.(jpg|jpeg|png|gif|webp|svg)(\?|$)', url, re.IGNORECASE):
            image_tag = f"[IMAGE:{url}]"
            formatted_text = formatted_text.replace(f"({url})", "")
            formatted_text = formatted_text.replace(url, "")
            formatted_text += f"\n\n{image_tag}"
            has_images = True

    return formatted_text.strip(), has_images

# Context prompt builder
def build_full_prompt(system_prompt, memory, message):
    context = "\n".join(f"{msg.role.value}: {msg.content}" for msg in memory.get_all()[-6:])
    return f"""{system_prompt}

L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
{context}

C√ÇU H·ªéI HI·ªÜN T·∫†I: {message}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh cu·ªôc tr√≤ chuy·ªán v√† th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu.
""" if context else f"{system_prompt}\n\nC√ÇU H·ªéI: {message}"

# Multi index engine
class MultiIndexChatEngine:
    def __init__(self, indices_config, llm):
        self.llm = llm
        self.memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
        tools = []

        for config in indices_config:
            query_engine = config['index'].as_query_engine(
                llm=llm,
                similarity_top_k=3  # Limit to top 3 most similar results
            )
            tools.append(QueryEngineTool(
                query_engine=query_engine,
                metadata=ToolMetadata(name=config['name'], description=config['description'])
            ))

        self.router = RouterQueryEngine(
            selector=LLMSingleSelector.from_defaults(llm=llm),
            query_engine_tools=tools,
            verbose=True
        )

        self.system_prompt = """
        B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªÅ s·∫£n ph·∫©m c·ªßa C√îNG TY C·ªî PH·∫¶N C√îNG NGH·ªÜ SINH H·ªåC HO√Ä B√åNH 
        
        NHI·ªÜM V·ª§:
        - T∆∞ v·∫•n s·∫£n ph·∫©m m·ªôt c√°ch chi ti·∫øt v√† h·ªØu √≠ch
        - Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ th√†nh ph·∫ßn, c√¥ng d·ª•ng, c√°ch s·ª≠ d·ª•ng
        - Cung c·∫•p th√¥ng tin v·ªÅ gi√° c·∫£ v√† n∆°i mua n·∫øu c√≥
        - Duy tr√¨ cu·ªôc tr√≤ chuy·ªán t·ª± nhi√™n v√† th√¢n thi·ªán
        - Khi c√≥ h√¨nh ·∫£nh s·∫£n ph·∫©m trong d·ªØ li·ªáu, H√ÉY LU√îN BAO G·ªíM trong c√¢u tr·∫£ l·ªùi
        - ƒê·ªãnh d·∫°ng h√¨nh ·∫£nh theo chu·∫©n ·∫¢nh:URL ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi
        
        QUY T·∫ÆC V·ªÄ H√åNH ·∫¢NH:
        - Lu√¥n ki·ªÉm tra d·ªØ li·ªáu c√≥ ch·ª©a h√¨nh ·∫£nh s·∫£n ph·∫©m kh√¥ng
        - N·∫øu c√≥ h√¨nh ·∫£nh ph√π h·ª£p v·ªõi c√¢u h·ªèi, h√£y th√™m v√†o c√¢u tr·∫£ l·ªùi
        - M·ªói h√¨nh ·∫£nh ph·∫£i c√≥ m√¥ t·∫£ ng·∫Øn g·ªçn tr∆∞·ªõc khi hi·ªÉn th·ªã
        - Gi·ªõi h·∫°n t·ªëi ƒëa 3 h√¨nh ·∫£nh m·ªói c√¢u tr·∫£ l·ªùi
        
        PHONG C√ÅCH:
        - S·ª≠ d·ª•ng ti·∫øng Vi·ªát t·ª± nhi√™n, kh√¥ng c·ª©ng nh·∫Øc
        - ƒê∆∞a ra l·ªùi khuy√™n c·ª• th·ªÉ v√† th·ª±c t·∫ø
        - H·ªèi l·∫°i n·∫øu c·∫ßn l√†m r√µ y√™u c·∫ßu c·ªßa kh√°ch h√†ng
        """

    def chat_stream(self, message: str):
        prompt = build_full_prompt(self.system_prompt, self.memory, message)
        response = str(self.router.query(prompt))
        formatted, _ = extract_and_format_images(response)

        self.memory.put(ChatMessage(role=MessageRole.USER, content=message))
        self.memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=response))

        for i in range(0, len(formatted), 5):
            yield formatted[i:i+5]
            time.sleep(0.02)

    def reset(self):
        self.memory.reset()

# RAG setup
def setup_rag_system():
    llm = GoogleGenAI(
        model="gemini-2.0-flash",
    )

    # llm = DeepSeek(model="deepseek-chat")
    
    # llm=Groq(
    #     # model="meta-llama/llama-4-maverick-17b-128e-instruct", # k ·ªïn
    #     model="llama-3.3-70b-versatile",
    #     # model="llama3-70b-8192", # ·ªïn nh·∫•t
    #     temperature=0.1,
    #     # max_tokens=4000
    # )

    # llm = Cerebras(
    #     # model="llama-3.1-8b", 
    #     model="llama-3.3-70b",  
    #     # model="qwen-3-32b", # think
    #     # model="llama-4-scout-17b-16e-instruct",  # kh√¥ng ·ªïn
    #     api_key=os.environ["CEREBRAS_API_KEY"])



    # index_oniz = LlamaCloudIndex(
    #     name="subsequent-stork-2025-05-22",
    #     project_name="Default", 
    #     organization_id="1bcf5fb2-bd7d-4c76-b6d5-bb793486e1b3",
    #     api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
    # )

    # index_excel = LlamaCloudIndex(
    #     name="funny-clownfish-2025-05-28",
    #     project_name="Default", 
    #     organization_id="1bcf5fb2-bd7d-4c76-b6d5-bb793486e1b3",
    #     api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
    # )

    index_pinecone = setup_pinecone_vector_index()
    
    indices_config = [
        {
            'name': 'Th√¥ng tin v·ªÅ C√îNG TY C·ªî PH·∫¶N C√îNG NGH·ªÜ SINH H·ªåC HO√Ä B√åNH',
            'index': index_pinecone,
            'description': "C√îNG TY C·ªî PH·∫¶N C√îNG NGH·ªÜ SINH H·ªåC HO√Ä B√åNH"
        }
    ]

    return MultiIndexChatEngine(indices_config, llm)

# Kh·ªüi t·∫°o chat engine
chat_engine = setup_rag_system()

# T·∫°o router - kh√¥ng ƒë·∫∑t prefix ·ªü ƒë√¢y, s·∫Ω ƒë·∫∑t trong main_test.py
chat_router = APIRouter()

@chat_router.post("/chat")
async def chat_endpoint(input: ChatInput):
    def generate():
        try:
            yield from chat_engine.chat_stream(input.message)
        except Exception as e:
            yield f"‚ùå L·ªói: {str(e)}"
    return StreamingResponse(generate(), media_type="text/plain; charset=utf-8")

@chat_router.post("/reset")
async def reset_chat():
    try:
        chat_engine.reset()
        return {"message": "ƒê√£ reset l·ªãch s·ª≠ tr√≤ chuy·ªán th√†nh c√¥ng"}
    except Exception as e:
        return {"error": str(e)}

@chat_router.get("/")
async def root():
    return {
        "message": "ü§ñ Oniiz RAG Chat Server ƒëang ho·∫°t ƒë·ªông",
        "features": ["Image Support", "Streaming Chat", "Memory Management"],
        "endpoints": {
            "/chat": "POST - Chat v·ªõi streaming response",
            "/reset": "POST - Reset l·ªãch s·ª≠",
            "/health": "GET - Ki·ªÉm tra tr·∫°ng th√°i"
        }
    }

@chat_router.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "vector_store": "pinecone",
        "features": ["streaming", "memory", "images"],
    }