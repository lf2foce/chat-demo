{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "import faiss\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from markitdown import MarkItDown\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Khởi tạo các client\n",
    "gemini_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# OpenAI Client\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Cerebras Client\n",
    "cerebras_client = OpenAI(\n",
    "    api_key=os.getenv(\"CEREBRAS_API_KEY\"),\n",
    "    base_url=\"https://api.cerebras.ai/v1\"\n",
    ")\n",
    "\n",
    "# Groq Client\n",
    "groq_client = OpenAI(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "# FPT Embedding configuration\n",
    "API_KEY_FPT = os.getenv(\"FPT_API_KEY\", \"sk-f4MwNZO_2TtcdBYAwzq5rw\")\n",
    "BASE_URL_FPT = \"https://mkp-api.fptcloud.com\"\n",
    "MODEL_FPT = \"Vietnamese_Embedding\"\n",
    "\n",
    "# Client cho FPT Embedding\n",
    "fpt_embedding_client = OpenAI(\n",
    "    api_key=API_KEY_FPT,\n",
    "    base_url=BASE_URL_FPT\n",
    ")\n",
    "\n",
    "# Initialize MarkItDown with plugins disabled for stability\n",
    "markitdown = MarkItDown(enable_plugins=False)\n",
    "\n",
    "def get_embedding(text: str, model: str = MODEL_FPT) -> list:\n",
    "    \"\"\"\n",
    "    Retrieves the embedding for a given text using the specified model.\n",
    "    Args:\n",
    "        text (str): The input text to embed.\n",
    "        model (str): The model to use for embedding.\n",
    "    Returns:\n",
    "        list: The embedding vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = fpt_embedding_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=model\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_with_markitdown(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from various file formats using Microsoft MarkItDown\n",
    "    Supports: PDF, DOCX, PPTX, XLSX, CSV, HTML, images, audio, and more\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        result = markitdown.convert(file_path)\n",
    "        \n",
    "        if result and result.text_content:\n",
    "            print(f\"Successfully extracted {len(result.text_content)} characters\")\n",
    "            return result.text_content\n",
    "        else:\n",
    "            print(\"No content extracted from file\")\n",
    "            return \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def split_text(text, chunk_size=800, overlap=100):\n",
    "    \"\"\"\n",
    "    Split text into chunks with overlap\n",
    "    Increased chunk size for better context with MarkItDown's structured output\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "        \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Try to break at sentence boundaries\n",
    "        if end < len(text):\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_newline = chunk.rfind('\\n')\n",
    "            break_point = max(last_period, last_newline)\n",
    "            \n",
    "            if break_point > start + chunk_size * 0.5:  # At least half the chunk\n",
    "                chunk = text[start:break_point + 1]\n",
    "                end = break_point + 1\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        start = end - overlap\n",
    "        \n",
    "    return [chunk for chunk in chunks if chunk]  # Filter empty chunks\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"Create FAISS index from embeddings\"\"\"\n",
    "    if not embeddings or not embeddings[0]:\n",
    "        return None\n",
    "        \n",
    "    dimension = len(embeddings[0])\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # Filter out None embeddings\n",
    "    valid_embeddings = [emb for emb in embeddings if emb is not None]\n",
    "    if valid_embeddings:\n",
    "        index.add(np.array(valid_embeddings).astype('float32'))\n",
    "        return index\n",
    "    return None\n",
    "\n",
    "def search_faiss_index(index, query_embedding, k=5):\n",
    "    \"\"\"Search FAISS index for similar vectors\"\"\"\n",
    "    if not index or not query_embedding:\n",
    "        return None, None\n",
    "        \n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    return distances, indices\n",
    "\n",
    "def generate_response(prompt, context, provider=\"cerebras\"):\n",
    "    \"\"\"Generate response using specified provider\"\"\"\n",
    "    # Chọn client và model dựa trên provider\n",
    "    if provider == \"cerebras\":\n",
    "        client = cerebras_client\n",
    "        model = \"llama-4-scout-17b-16e-instruct\"\n",
    "    elif provider == \"groq\":\n",
    "        client = groq_client\n",
    "        model = \"llama3.1-8b\"\n",
    "    elif provider == \"openai\":\n",
    "        client = openai_client\n",
    "        model = \"gpt-4o-mini\"\n",
    "    else:\n",
    "        print(f\"Unknown provider: {provider}, using cerebras as default\")\n",
    "        client = cerebras_client\n",
    "        model = \"llama-4-scout-17b-16e-instruct\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a helpful assistant. Answer questions based on the provided context. If the context doesn't contain relevant information, say so clearly. Answer in Vietnamese if the question is in Vietnamese.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Question: {prompt}\\n\\nContext:\\n{context}\"\n",
    "                }\n",
    "            ],\n",
    "            stream=False,\n",
    "            temperature=0.7,\n",
    "            max_tokens=1500\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response with {provider}: {e}\"\n",
    "\n",
    "def display_results(chunks, indices, distances=None):\n",
    "    \"\"\"Display retrieved chunks with relevance scores\"\"\"\n",
    "    if not indices or len(indices) == 0:\n",
    "        print(\"No relevant chunks found\")\n",
    "        return\n",
    "        \n",
    "    print(\"Retrieved relevant chunks:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(chunks):\n",
    "            relevance_score = \"\"\n",
    "            if distances is not None and len(distances[0]) > i:\n",
    "                score = distances[0][i]\n",
    "                relevance_score = f\" (Relevance: {1/(1+score):.3f})\"\n",
    "            \n",
    "            print(f\"Chunk {i+1}{relevance_score}:\")\n",
    "            print(chunks[idx])\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    RAG System class using Microsoft MarkItDown for document parsing\n",
    "    Supports multiple file formats: PDF, DOCX, PPTX, XLSX, CSV, HTML, images, audio, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client=None, llm_model=None):\n",
    "        \"\"\"\n",
    "        Initialize RAG system\n",
    "        Args:\n",
    "            llm_client: OpenAI client for image description (optional)\n",
    "            llm_model: Model name for image description (optional)\n",
    "        \"\"\"\n",
    "        self.chunks = []\n",
    "        self.embeddings = []\n",
    "        self.index = None\n",
    "        self.valid_indices = []  # Track which chunks have valid embeddings\n",
    "        \n",
    "        # Initialize MarkItDown with optional LLM for image descriptions\n",
    "        if llm_client and llm_model:\n",
    "            self.markitdown = MarkItDown(\n",
    "                llm_client=llm_client, \n",
    "                llm_model=llm_model,\n",
    "                enable_plugins=False\n",
    "            )\n",
    "            print(\"MarkItDown initialized with LLM support for image descriptions\")\n",
    "        else:\n",
    "            self.markitdown = MarkItDown(enable_plugins=False)\n",
    "            print(\"MarkItDown initialized (basic mode)\")\n",
    "    \n",
    "    def load_document(self, file_path):\n",
    "        \"\"\"\n",
    "        Load document from file path using MarkItDown\n",
    "        Supports: PDF, DOCX, PPTX, XLSX, CSV, HTML, images, audio, ZIP, etc.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Extract text using MarkItDown\n",
    "        text = extract_text_with_markitdown(file_path)\n",
    "        \n",
    "        if not text:\n",
    "            print(\"No text extracted from file\")\n",
    "            return False\n",
    "        \n",
    "        # Split text into chunks\n",
    "        self.chunks = split_text(text)\n",
    "        print(f\"Document split into {len(self.chunks)} chunks\")\n",
    "        \n",
    "        if not self.chunks:\n",
    "            print(\"No chunks created\")\n",
    "            return False\n",
    "        \n",
    "        # Create embeddings\n",
    "        print(\"Creating embeddings...\")\n",
    "        self.embeddings = []\n",
    "        self.valid_indices = []\n",
    "        \n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Processing chunk {i+1}/{len(self.chunks)}\")\n",
    "            \n",
    "            embedding = get_embedding(chunk)\n",
    "            if embedding:\n",
    "                self.embeddings.append(embedding)\n",
    "                self.valid_indices.append(i)\n",
    "            else:\n",
    "                print(f\"Failed to create embedding for chunk {i+1}\")\n",
    "        \n",
    "        if not self.embeddings:\n",
    "            print(\"No valid embeddings created\")\n",
    "            return False\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.index = create_faiss_index(self.embeddings)\n",
    "        if self.index:\n",
    "            print(f\"FAISS index created successfully with {len(self.embeddings)} embeddings!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Failed to create FAISS index\")\n",
    "            return False\n",
    "    \n",
    "    def query(self, question, provider=\"cerebras\", k=3, show_context=True):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        if not self.index:\n",
    "            return \"Please load a document first using load_document()\"\n",
    "        \n",
    "        if not question.strip():\n",
    "            return \"Please provide a valid question\"\n",
    "        \n",
    "        print(f\"Searching for: {question}\")\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = get_embedding(question)\n",
    "        if not query_embedding:\n",
    "            return \"Error: Could not create embedding for the question\"\n",
    "        \n",
    "        # Search for similar chunks\n",
    "        distances, indices = search_faiss_index(self.index, query_embedding, k=k)\n",
    "        \n",
    "        if distances is None or indices is None:\n",
    "            return \"Error: Search failed\"\n",
    "        \n",
    "        # Map back to original chunk indices\n",
    "        retrieved_chunks = []\n",
    "        actual_indices = []\n",
    "        \n",
    "        for idx in indices[0]:\n",
    "            if idx < len(self.valid_indices):\n",
    "                original_idx = self.valid_indices[idx]\n",
    "                retrieved_chunks.append(self.chunks[original_idx])\n",
    "                actual_indices.append([original_idx])\n",
    "        \n",
    "        if not retrieved_chunks:\n",
    "            return \"No relevant information found\"\n",
    "        \n",
    "        context = \"\\n\\n\".join(retrieved_chunks)\n",
    "        \n",
    "        if show_context:\n",
    "            display_results(self.chunks, [actual_indices[0]], distances)\n",
    "        \n",
    "        # Generate response\n",
    "        print(f\"\\nGenerating response using {provider}...\")\n",
    "        response = generate_response(question, context, provider)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_document_info(self):\n",
    "        \"\"\"Get information about the loaded document\"\"\"\n",
    "        if not self.chunks:\n",
    "            return \"No document loaded\"\n",
    "        \n",
    "        total_chars = sum(len(chunk) for chunk in self.chunks)\n",
    "        valid_embeddings = len(self.embeddings)\n",
    "        \n",
    "        return f\"\"\"\n",
    "Document Information:\n",
    "- Total chunks: {len(self.chunks)}\n",
    "- Valid embeddings: {valid_embeddings}\n",
    "- Total characters: {total_chars:,}\n",
    "- Average chunk size: {total_chars // len(self.chunks) if self.chunks else 0} characters\n",
    "- Index ready: {'Yes' if self.index else 'No'}\n",
    "        \"\"\".strip()\n",
    "\n",
    "# Example usage for Jupyter Notebook:\n",
    "print(\"RAG System with Microsoft MarkItDown initialized!\")\n",
    "print(\"\\nSupported file formats:\")\n",
    "print(\"- PDF, DOCX, PPTX, XLSX, CSV\")\n",
    "print(\"- HTML, TXT, JSON, XML\")\n",
    "print(\"- Images (with OCR), Audio (with transcription)\")\n",
    "print(\"- ZIP files, YouTube URLs, EPubs\")\n",
    "print(\"\\nUsage example:\")\n",
    "print(\"# Create RAG system\")\n",
    "print(\"rag = RAGSystem()\")\n",
    "print(\"\\n# For image descriptions, use:\")\n",
    "print(\"# from openai import OpenAI\")\n",
    "print(\"# client = OpenAI()\")\n",
    "print(\"# rag = RAGSystem(llm_client=client, llm_model='gpt-4o')\")\n",
    "print(\"\\n# Load document (any supported format)\")\n",
    "print(\"rag.load_document('university_texts.csv')\")\n",
    "print(\"# or\")\n",
    "print(\"rag.load_document('document.pdf')\")\n",
    "print(\"\\n# Query the system\") \n",
    "print(\"response = rag.query('Your question here', provider='cerebras')\")\n",
    "print(\"print(response)\")\n",
    "print(\"\\n# Get document info\")\n",
    "print(\"print(rag.get_document_info())\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
