# advanced ƒëang fix
import os
import re
import time
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

# LlamaIndex
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.base.llms.types import ChatMessage, MessageRole
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.selectors import LLMSingleSelector

# from llama_index.llms.deepseek import DeepSeek
# from llama_index.llms.groq import Groq
# from llama_index.llms.cerebras import Cerebras
from llama_index.indices.managed.llama_cloud import LlamaCloudIndex

from my_module import setup_pinecone_vector_index

# Embed model setup
Settings.embed_model = GoogleGenAIEmbedding(
    model_name="gemini-embedding-exp-03-07",
    api_key=os.getenv("GOOGLE_API_KEY")
)

# Chat input model
class ChatInput(BaseModel):
    message: str


def extract_and_format_images(text: str):
    """
    T√¨m v√† format c√°c URL h√¨nh ·∫£nh trong text d·∫°ng (https://...) th√†nh [IMAGE:...]
    ·∫®n link g·ªëc v√† thay b·∫±ng placeholder ·∫£nh cho frontend x·ª≠ l√Ω.
    Returns: (formatted_text, has_images)
    """
    # Regex t√¨m (https://...) trong ngo·∫∑c ƒë∆°n ho·∫∑c ngo√†i
    url_pattern = r'\(?(https?://[^\s\)]+)\)?'

    formatted_text = text
    has_images = False
    found_urls = re.findall(url_pattern, text)

    for url in found_urls:
        # N·∫øu ch∆∞a ph·∫£i ·∫£nh, nh∆∞ng t·ª´ c√°c d·ªãch v·ª• hosting (UploadThing, S3, Cloudinary‚Ä¶)
        if any(domain in url for domain in ["uploadthing", "amazonaws.com", "ufs.sh", "supabase", "cdn", "unsplash"]):
            image_tag = f"[IMAGE:{url}]"
            # Replace ƒëo·∫°n "(https://...)" ho·∫∑c " https://... "
            formatted_text = formatted_text.replace(f"({url})", "")  # x√≥a trong ngo·∫∑c
            formatted_text = formatted_text.replace(url, "")         # x√≥a n·∫øu kh√¥ng ngo·∫∑c
            formatted_text += f"\n\n{image_tag}"
            has_images = True
        elif re.search(r'\.(jpg|jpeg|png|gif|webp|svg)(\?|$)', url, re.IGNORECASE):
            image_tag = f"[IMAGE:{url}]"
            formatted_text = formatted_text.replace(f"({url})", "")
            formatted_text = formatted_text.replace(url, "")
            formatted_text += f"\n\n{image_tag}"
            has_images = True

    return formatted_text.strip(), has_images

# Context prompt builder
def build_full_prompt(system_prompt, memory, message):
    context = "\n".join(f"{msg.role.value}: {msg.content}" for msg in memory.get_all()[-6:])
    return f"""{system_prompt}

L·ªäCH S·ª¨ TR√í CHUY·ªÜN:
{context}

C√ÇU H·ªéI HI·ªÜN T·∫†I: {message}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ng·ªØ c·∫£nh cu·ªôc tr√≤ chuy·ªán v√† th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu.
""" if context else f"{system_prompt}\n\nC√ÇU H·ªéI: {message}"

# Multi index engine
class MultiIndexChatEngine:
    def __init__(self, indices_config, llm):
        self.llm = llm
        self.memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
        tools = []

        for config in indices_config:
            query_engine = config['index'].as_query_engine(
                llm=llm,
                similarity_top_k=3  # Limit to top 3 most similar results
            )
            tools.append(QueryEngineTool(
                query_engine=query_engine,
                metadata=ToolMetadata(name=config['name'], description=config['description'])
            ))

        self.router = RouterQueryEngine(
            selector=LLMSingleSelector.from_defaults(llm=llm),
            query_engine_tools=tools,
            verbose=True
        )

        self.system_prompt = """
           B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªÅ gi√°o d·ª•c v√† c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc t·∫°i Vi·ªát Nam.
        
        NHI·ªÜM V·ª§:
        - T∆∞ v·∫•n th√¥ng tin v·ªÅ tr∆∞·ªùng ƒë·∫°i h·ªçc
        - Tr·∫£ l·ªùi v·ªÅ ƒëi·ªÉm chu·∫©n, ng√†nh h·ªçc, tuy·ªÉn sinh
        - Th·ªëng k√™ v√† so s√°nh gi·ªØa c√°c tr∆∞·ªùng
        - T√¨m ki·∫øm tr∆∞·ªùng ph√π h·ª£p v·ªõi nhu c·∫ßu
        - H·ªó tr·ª£ decision making cho h·ªçc sinh
        
        KH·∫¢ NƒÇNG ƒê·∫∂C BI·ªÜT:
        - Structured queries: th·ªëng k√™, so s√°nh, ranking
        - Text search: t√¨m ki·∫øm theo t·ª´ kh√≥a t·ª± nhi√™n
        - Hybrid approach: k·∫øt h·ª£p c·∫£ hai ph∆∞∆°ng ph√°p
        
        PHONG C√ÅCH:
        - Ti·∫øng Vi·ªát t·ª± nhi√™n, th√¢n thi·ªán
        - Th√¥ng tin ch√≠nh x√°c, c·∫≠p nh·∫≠t
        - L·ªùi khuy√™n thi·∫øt th·ª±c
        - Gi·∫£i th√≠ch r√µ r√†ng
        
        EXAMPLES:
        - "So s√°nh ƒëi·ªÉm chu·∫©n UIT v√† PNTU" ‚Üí Structured query
        - "T√¨m tr∆∞·ªùng y khoa ·ªü TP.HCM" ‚Üí Text search  
        - "Top 5 tr∆∞·ªùng ƒëi·ªÉm chu·∫©n cao nh·∫•t" ‚Üí Structured query
        """

    def chat_stream(self, message: str):
        prompt = build_full_prompt(self.system_prompt, self.memory, message)
        response = str(self.router.query(prompt))
        formatted, _ = extract_and_format_images(response)

        self.memory.put(ChatMessage(role=MessageRole.USER, content=message))
        self.memory.put(ChatMessage(role=MessageRole.ASSISTANT, content=response))

        yield formatted
        # Optional: if a small delay is still desired for perceived streaming, add it here.
        # time.sleep(0.02) # This might not be necessary if yielding the whole string.

    def reset(self):
        self.memory.reset()

# RAG setup
def setup_rag_system():
    llm = GoogleGenAI(
        model="gemini-2.0-flash",
    )

    # llm = DeepSeek(model="deepseek-chat")
    
    # llm=Groq(
    #     # model="meta-llama/llama-4-maverick-17b-128e-instruct", # k ·ªïn
    #     model="llama-3.3-70b-versatile",
    #     # model="llama3-70b-8192", # ·ªïn nh·∫•t
    #     temperature=0.1,
    #     # max_tokens=4000
    # )

    # llm = Cerebras(
    #     # model="llama-3.1-8b", 
    #     model="llama-3.3-70b",  
    #     # model="qwen-3-32b", # think
    #     # model="llama-4-scout-17b-16e-instruct",  # kh√¥ng ·ªïn
    #     api_key=os.environ["CEREBRAS_API_KEY"])



 

    index_dsdaihoc = LlamaCloudIndex(
        name="dsdaihoc",
        project_name="Default",
        organization_id="1bcf5fb2-bd7d-4c76-b6d5-bb793486e1b3",
        api_key=os.getenv("LLAMA_CLOUD_API_KEY"),
        )


    # index_pinecone = setup_pinecone_vector_index()
    
    indices_config = [
        # {
        #     'name': 'Th√¥ng tin v·ªÅ c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc t·∫°i Vi·ªát Nam',
        #     'index': index_pinecone,
        #     'description': "C√°c tr∆∞·ªùng ƒë·∫°i h·ªçc t·∫°i Vi·ªát Nam"
        # },
        {
            'name': 'Th√¥ng tin v·ªÅ c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc t·∫°i Vi·ªát Nam',
            'index': index_dsdaihoc,
            'description': "C√°c tr∆∞·ªùng ƒë·∫°i h·ªçc t·∫°i Vi·ªát Nam"
        }
    ]

    return MultiIndexChatEngine(indices_config, llm)

# Kh·ªüi t·∫°o chat engine
chat_engine = setup_rag_system()

# T·∫°o router - kh√¥ng ƒë·∫∑t prefix ·ªü ƒë√¢y, s·∫Ω ƒë·∫∑t trong main_test.py
chat_router = APIRouter()

@chat_router.post("/chat")
async def chat_endpoint(input: ChatInput):
    def generate():
        try:
            yield from chat_engine.chat_stream(input.message)
        except Exception as e:
            yield f"‚ùå L·ªói: {str(e)}"
    return StreamingResponse(generate(), media_type="text/plain; charset=utf-8")

@chat_router.post("/reset")
async def reset_chat():
    try:
        chat_engine.reset()
        return {"message": "ƒê√£ reset l·ªãch s·ª≠ tr√≤ chuy·ªán th√†nh c√¥ng"}
    except Exception as e:
        return {"error": str(e)}

@chat_router.get("/")
async def root():
    return {
        "message": "ü§ñ Oniiz RAG Chat Server ƒëang ho·∫°t ƒë·ªông",
        "features": ["Image Support", "Streaming Chat", "Memory Management"],
        "endpoints": {
            "/chat": "POST - Chat v·ªõi streaming response",
            "/reset": "POST - Reset l·ªãch s·ª≠",
            "/health": "GET - Ki·ªÉm tra tr·∫°ng th√°i"
        }
    }

@chat_router.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "vector_store": "pinecone",
        "features": ["streaming", "memory", "images"],
    }